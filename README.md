# Perplexity-2.0-ChatBot

# ðŸ¤– OpenAI LLaMA3 Chatbot (Fullstack)

A fullstack chatbot application powered by [Ollama](https://ollama.com/) and LLaMA3, with a React + Vite frontend and a Node.js + Express backend. The backend uses your local GPU to run LLaMA3 efficiently and cost-free!

---

## ðŸš€ Features

- ðŸ’¬ AI chatbot using LLaMA3 via Ollama
- ðŸŽ¯ Frontend built with React & TailwindCSS (Vite)
- âš™ï¸ Backend built with Node.js + Express
- ðŸ” Secure environment with `.env` support
- ðŸ”„ Connects locally via REST API (`/api/chat`)

---

## ðŸ“ Project Structure

openai-bot/
â”œâ”€â”€ backend/ # Node.js + Express backend (Ollama client)
â”‚ â”œâ”€â”€ server.js
â”‚ â”œâ”€â”€ .env # Contains OPENAI_API_KEY (optional)
â”‚ â””â”€â”€ package.json
â”œâ”€â”€ frontend/ # React + Vite frontend
â”‚ â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ tailwind.config.js
â”‚ â””â”€â”€ package.json
â””â”€â”€ README.md


---

## âš™ï¸ Getting Started

### ðŸ§  Prerequisites

- [Node.js](https://nodejs.org/) (v18+ recommended)
- [Ollama](https://ollama.com/) (local LLaMA3 model runner)
- Git installed

### ðŸ› ï¸ Backend Setup

```bash
cd backend
npm install
# Add your environment variable
echo PORT=3000 >> .env
# Start the server
npm run dev

Frontend Setup

cd frontend
npm install
npm run dev

Local GPU Inference with Ollama

Make sure you have pulled the model:

ollama pull llama3
ollama serve
